{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZQ1xRp1gn4N"
      },
      "source": [
        "# The Fundamentals of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNOfwHr4g0cm"
      },
      "source": [
        "## Chapter 1 - The Machine Learning Landscape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFpWFoE7qIXI"
      },
      "source": [
        "### **What Is Machine Learning?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol2Dd9TToigx"
      },
      "source": [
        "Machine Learning is the science (and art) of programming computers so they can\n",
        "learn from data.\n",
        "\n",
        "Here is a slightly more general definition:\n",
        "\n",
        "\n",
        "> [Machine Learning is the] field of study that gives computers the ability to learn\n",
        "without being explicitly programmed.<br>&nbsp;&nbsp;\n",
        " —Arthur Samuel, 1959\n",
        "\n",
        "And a more engineering-oriented one:\n",
        "\n",
        "> A computer program is said to learn from experience E with respect to some task T\n",
        "and some performance measure P, if its performance on T, as measured by P,\n",
        "improves with experience E.<br>&nbsp;&nbsp; —Tom Mitchell, 1997\n",
        "\n",
        "A spam filter is a Machine Learning program that learns from examples of spam and ham emails.\n",
        "\n",
        "The dataset used for learning is the training set, and each example is a training instance.\n",
        "\n",
        "In this case:\n",
        "\n",
        "- Task (T): flag spam for new emails.\n",
        "\n",
        "- Experience (E): the training data.\n",
        "\n",
        "- Performance (P): ratio of correctly classified emails (accuracy).\n",
        "\n",
        "Simply downloading Wikipedia adds data, but it is not Machine Learning since no T, E, or P are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL9kdlfBqoFd"
      },
      "source": [
        "### **Why Use Machine Learning?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuXlMCDKqxcV"
      },
      "source": [
        "Consider how you would write a spam filter using traditional programming techniques (Figure 1-1):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Un42XCrBF3"
      },
      "source": [
        "1. Identify common spam features, such as frequent words (“4U,” “credit card,” “free,” “amazing”) and patterns in the subject, sender, or body.\n",
        "\n",
        "2. Create detection algorithms for these patterns and flag emails as spam if enough are present.\n",
        "\n",
        "3. Test and refine the program until performance is satisfactory.\n",
        "\n",
        "![Figure 1-1](./Fig/Chapter_1/Fig1-1.png)\n",
        "\n",
        "\n",
        "Rule-based spam filters become long, complex, and hard to maintain. In contrast, a Machine Learning–based filter (Figure 1-2) automatically learns frequent word patterns in spam vs. ham, making it shorter, easier to maintain, and more accurate. When spammers adapt (e.g., changing “4U” to “For U”), rule-based filters require constant updates, while an ML filter (Figure 1-3) adapts automatically by detecting new frequent patterns in flagged spam.\n",
        "\n",
        "![Figure 1-2](./Fig/Chapter_1/Fig1-2.png)\n",
        "\n",
        "![Figure 1-3](./Fig/Chapter_1/Fig1-3.png)\n",
        "\n",
        "Machine Learning excels at problems too complex or lacking known algorithms, such as speech recognition. Simple hardcoded rules (e.g., detecting the high-pitch “T” in “two”) cannot scale to many words, speakers, noisy environments, and languages. Instead, ML algorithms learn from large sets of recordings. ML can also help humans learn (Figure 1-4): trained models can be inspected to reveal key predictors (e.g., words in spam filters), sometimes uncovering unexpected correlations or trends. Using ML to explore large datasets and reveal hidden patterns is known as data mining.\n",
        "\n",
        "![Figure 1-4](./Fig/Chapter_1/Fig1-4.png)\n",
        "\n",
        "To summarize, Machine Learning is well-suited for:\n",
        "\n",
        "- Problems where traditional solutions need heavy fine-tuning or long rule lists.\n",
        "\n",
        "- Complex problems with no good traditional solution.\n",
        "\n",
        "- Fluctuating environments where systems must adapt to new data.\n",
        "\n",
        "- Gaining insights from complex problems and large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Example of Applications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s look at some concrete examples of Machine Learning tasks, along with the tech‐\n",
        "niques that can tackle them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Analyzing images of products on a production line to automatically classify them***\n",
        "<br>\n",
        "&nbsp;&nbsp; This is image classification, typically performed using convolutional neural net‐\n",
        "works (CNNs; see Chapter 14).\n",
        "\n",
        "***Detecting tumors in brain scans***\n",
        "<br>\n",
        "&nbsp;&nbsp; This is semantic segmentation, where each pixel in the image is classified (as we\n",
        "want to determine the exact location and shape of tumors), typically using CNNs\n",
        "as well.\n",
        "\n",
        "***Automatically classifying news articles***\n",
        "<br>\n",
        "&nbsp;&nbsp; This is natural language processing (NLP), and more specifically text classifica‐\n",
        "tion, which can be tackled using recurrent neural networks (RNNs), CNNs, or\n",
        "Transformers (see Chapter 16).\n",
        "\n",
        "***Automatically flagging offensive comments on discussion forums***\n",
        "<br>\n",
        "&nbsp;&nbsp; This is also text classification, using the same NLP tools.\n",
        "\n",
        "***Summarizing long documents automatically***\n",
        "<br>\n",
        "&nbsp;&nbsp; This is a branch of NLP called text summarization, again using the same tools.\n",
        "\n",
        "***Creating a chatbot or a personal assistant***\n",
        "<br>\n",
        "&nbsp; &nbsp; This involves many NLP components, including natural language understanding\n",
        "(NLU) and question-answering modules.\n",
        "\n",
        "***Building an intelligent bot for a game***\n",
        "<br>\n",
        "<p align = \"justify\">\n",
        "&nbsp;&nbsp;This is often tackled using Reinforcement Learning (RL; see Chapter 18),which\n",
        "is a branch of Machine Learning that trains agents (such as bots) to pick the\n",
        "actions that will maximize their rewards over time (e.g., a bot may get a reward\n",
        "every time the player loses some life points), within a given environment (such as\n",
        "the game). The famous AlphaGo program that beat the world champion at the\n",
        "game of Go was built using RL.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Supervised Learning / Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Machine Learning systems can be classified by the type of supervision in training: supervised, unsupervised, semisupervised, and reinforcement learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Supervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In supervised learning, the training set includes labels (Figure 1-5). Common tasks:<br>\n",
        "<br>\n",
        "\n",
        "![Figure 1-5](./Fig/Fig1-5.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "- Classification (e.g., spam filter trained with labeled emails).\n",
        "- Regression (predicting numeric values such as car prices from features like mileage, age, brand; Figure 1-6).\n",
        "\n",
        "<br>\n",
        "\n",
        "![Figure 1-6](./Fig/Fig1-6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "Key notes:\n",
        "- Attribute = data type (e.g., mileage).\n",
        "- Feature = attribute + value (e.g., mileage = 15,000).\n",
        "- Some algorithms overlap (e.g., Logistic Regression is used for classification by outputting class probabilities).\n",
        "\n",
        "<br>    \n",
        "\n",
        "Important supervised learning algorithms:\n",
        "1. k-Nearest Neighbors\n",
        "2.Linear Regression\n",
        "3. Logistic Regression\n",
        "4. Support Vector Machines (SVMs)\n",
        "5. Decision Trees & Random Forests\n",
        "6. Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In unsupervised learning, the training data is unlabeled (Figure 1-7). The system learns without a teacher.\n",
        "<br>\n",
        "\n",
        "![Figure 1-7](./Fig/Chapter_1/Fig1-7.png)\n",
        "\n",
        "<br>\n",
        "Key unsupervised learning tasks and algorithms:\n",
        "\n",
        "- **Clustering**: K-Means, DBSCAN, Hierarchical Cluster Analysis (HCA).\n",
        "- **Anomaly & novelty detection**: One-class SVM, Isolation Forest.\n",
        "- **Visualization & dimensionality reduction**: PCA, Kernel PCA, LLE, t-SNE.\n",
        "- **Association rule learning**: Apriori, Eclat.\n",
        "\n",
        "**Example – Clustering** :<br>\n",
        "\n",
        "A clustering algorithm can group blog visitors without labels, e.g., 40% are comic book fans who read in the evening, while 20% are sci-fi fans active on weekends. Hierarchical clustering can subdivide groups further (Figure 1-8).\n",
        "\n",
        "<br>\n",
        "\n",
        "![Figure 1-8](./Fig/Chapter_1/Fig1-8.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example – Visualization** :<br>\n",
        "\n",
        "Visualization algorithms (e.g., t-SNE) project high-dimensional unlabeled data into 2D/3D while preserving structure, revealing semantic clusters (Figure 1-9).\n",
        "\n",
        "<br>\n",
        "\n",
        "![Figure 1-9](./Fig/Chapter_1/Fig1-9.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example – Dimensionality reduction** :<br>\n",
        "\n",
        "Simplifies data by merging correlated features (e.g., mileage + age → wear and tear). This reduces computation, storage, and can improve performance.\n",
        "\n",
        "**Example – Anomaly & novelty detection** :<br>\n",
        "\n",
        "Detects unusual patterns such as fraud or defects. The system learns normal instances, then flags unusual ones as anomalies (Figure 1-10). Novelty detection instead flags new instances not in the training set.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Figure 1-10](./Fig/Chapter_1/Fig1-10.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example – Association rule learning**:<br>\n",
        "\n",
        "Discovers relations in data, e.g., supermarket sales logs may show that people who buy barbecue sauce and potato chips also often buy steak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Semisupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since labeling data is costly, we often have many unlabeled and few labeled instances. Algorithms that can use both are called **semisupervised** learning (Figure 1-11).\n",
        "<br>\n",
        "\n",
        "![Figure 1-11](./Fig/Chapter_1/Fig1-11.png)\n",
        "\n",
        "Example :<br>\n",
        "Photo-hosting services (e.g., Google Photos) cluster faces (unsupervised), then need only one label per person to name them across all photos.\n",
        "\n",
        "How it works :<br>\n",
        "Most semisupervised approaches combine unsupervised and supervised methods. For instance, deep belief networks (DBNs) use unsupervised restricted Boltzmann machines (RBMs) stacked together, then fine-tuned with supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In **Reinforcement Learning (RL)**, the agent observes the environment, performs actions, and receives rewards or penalties (Figure 1-12). The goal is to learn the best policy a strategy mapping situations to actions that maximizes rewards over time.\n",
        "<br>\n",
        "\n",
        "![Figure 1-12](./Fig/Chapter_1/Fig1-12.png)\n",
        "\n",
        "Examples : <br>\n",
        "\n",
        "- Robots use RL to learn how to walk.\n",
        "- AlphaGo (DeepMind) learned its winning policy by analyzing millions of games and playing against itself, later beating world champion Ke Jie in 2017.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Batch and Online Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Batch Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In batch (offline) learning, the system is trained once on all available data, then deployed without further learning. To adapt to new data (e.g., new spam), the system must be retrained from scratch on the full dataset and replaced with the old version.\n",
        "\n",
        "Although training–evaluation–deployment can be automated (Figure 1-3), retraining is costly in time and resources. Typically, updates are done daily or weekly, which is unsuitable for rapidly changing data (e.g., stock prices).\n",
        "\n",
        "Batch learning also demands heavy resources (CPU, memory, disk, network). For massive datasets, frequent retraining can be too expensive or even infeasible. Moreover, systems with limited resources (e.g., smartphones, Mars rovers) cannot carry large datasets or train for hours.\n",
        "\n",
        "In such cases, incremental learning algorithms are a better option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Online Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In online learning, the system trains incrementally by receiving data sequentially, either one instance at a time or in small mini-batches. Each step is fast and cheap, allowing the model to adapt continuously as new data arrives.\n",
        "<br>\n",
        "\n",
        "![Figure 1-13](./Fig/Chapter_1/Fig1-13.png)\n",
        "\n",
        "\n",
        "This approach is ideal for continuous data streams (e.g., stock prices) or when resources are limited. Once new data is learned, it can be discarded, saving storage.\n",
        "\n",
        "Online learning also enables out-of-core learning, useful for huge datasets that don’t fit into memory. The algorithm processes data in chunks until the entire dataset has been covered.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Figure 1-14](./Fig/Chapter_1/Fig1-14.png)\n",
        "\n",
        "A key parameter is the learning rate:\n",
        "\n",
        "High rate → adapts quickly but forgets old data (e.g., a spam filter only catching recent spam).\n",
        "\n",
        "Low rate → adapts slowly, more robust to noise and outliers.\n",
        "\n",
        "The main risk is bad data, which can degrade performance over time (e.g., faulty sensors, spam attacks). To mitigate this, systems should be monitored closely, with mechanisms to pause learning, roll back to a stable state, or detect anomalies in the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Instance-Based Versus Model-Based Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Instance-Based Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One simple approach to generalization is instance-based learning—the system memorizes training examples and classifies new ones by comparing them to known instances using a similarity measure.\n",
        "\n",
        "For example, a spam filter could flag emails not only identical to known spam but also those with many words in common with spam messages. The new instance is then classified according to the most similar stored examples.\n",
        "<br>\n",
        "\n",
        "![Figure 1-15](./Fig/Chapter_1/Fig1-15.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model-Based Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another approach to generalization is model-based learning, where the system builds a model from examples and uses it to make predictions.\n",
        "<br>\n",
        "\n",
        "![Figure 1-16](./Fig/Chapter_1/Fig1-16.png)\n",
        "\n",
        "Example: Suppose you want to know if money makes people happy. You combine GDP per capita data (IMF) with life satisfaction data (OECD).\n",
        "<br>\n",
        "\n",
        "![Table 1-1](./Fig/Chapter_1/Table1-1.png)\n",
        "\n",
        "When plotted, the data shows a roughly linear trend: higher GDP per capita often corresponds to greater life satisfaction.\n",
        "<br>\n",
        "\n",
        "![Figure 1-17](./Fig/Chapter_1/Fig1-17.png)\n",
        "\n",
        "You can model this relationship using a linear function:\n",
        "\n",
        "> $life\\_satisfaction = \\theta_{0} + \\theta_{1} \\times GDP\\_per\\_capita$\n",
        "\n",
        "where $\\theta_{0}$ and $\\theta_{1}$ are parameters. Adjusting these parameters lets you fit different linear models.\n",
        "<br>\n",
        "\n",
        "![Figure 1-18](./Fig/Chapter_1/Fig1-18.png)\n",
        "\n",
        "To find the best parameters, you define a performance measure (e.g., a cost function measuring prediction error).  \n",
        "A Linear Regression algorithm then trains the model by minimizing this error.  \n",
        "For this example, the best fit is:\n",
        "\n",
        "> $\\theta_{0} = 4.85,\\ \\theta_{1} = 4.91 \\times 10^{-5}$\n",
        "\n",
        "![Figure 1-18](./Fig/Chapter_1/Fig1-18.png)\n",
        "\n",
        "Finally, you can use the trained model to predict unseen cases.  \n",
        "For instance, given Cyprus’s GDP per capita ($22,587), the model predicts:\n",
        "\n",
        "> $4.85 + (22,587 \\times 4.91 \\times 10^{-5}) \\approx 5.96$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example 1-1. Training and running a linear model using Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "\n",
        "# Load the data\n",
        "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
        "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
        "encoding='latin1', na_values=\"n/a\")\n",
        "\n",
        "# Prepare the data\n",
        "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
        "X = np.c_[country_stats[\"GDP per capita\"]]\n",
        "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
        "# Visualize the data\n",
        "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
        "plt.show()\n",
        "# Select a linear model\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "# Make a prediction for Cyprus\n",
        "X_new = [[22587]] # Cyprus's GDP per capita\n",
        "print(model.predict(X_new)) # outputs [[ 5.96242338]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model predicts poorly, you may need more attributes, better data, or a more powerful model (e.g., Polynomial Regression).\n",
        "\n",
        "In summary:\n",
        "\n",
        "- Study the data\n",
        "- Select a model\n",
        "- Train it (minimize cost)\n",
        "- Apply it to make predictions (inference)\n",
        "- This outlines a typical ML project workflow. Next, we’ll explore what can go wrong and hinder accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  **Insufficient Quantity of Training Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A toddler can learn what an apple is after seeing it just a few times, but Machine Learning still requires large amounts of data thousands of examples for simple tasks and millions for complex ones like image or speech recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **The Unreasonable Effectiveness of Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In 2001, Banko and Brill showed that different Machine Learning algorithms perform similarly well on natural language tasks when given enough data (see Figure 1-20).\n",
        "<br>\n",
        "\n",
        "![Figure 1-20](./Fig/Chapter_1/Fig1-20.png)\n",
        "\n",
        "<br>\n",
        "Their findings suggested prioritizing data collection over algorithm development. Peter Norvig later reinforced this idea in his 2009 paper “The Unreasonable Effectiveness of Data.” However, since small- and medium-sized datasets are still common, algorithms remain important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Nonrepresentative Training Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generalize well, training data must be representative of the cases you want to predict. For example, when missing countries were added to the earlier dataset (see Figure 1-21), the resulting model changed significantly, revealing that a simple linear model could not capture the relationship between wealth and happiness. Using nonrepresentative data leads to inaccurate predictions, especially for very poor or very rich countries. \n",
        "<br>\n",
        "\n",
        "![Figure 1-21](./Fig/Chapter_1/Fig1-21.png)\n",
        "\n",
        "<br>\n",
        "Ensuring representativeness is challenging, as small samples suffer from noise and large samples can still be biased if collected improperly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Examples of Sampling Bias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"text-align: justify;\">\n",
        "\n",
        "Perhaps the most famous example of sampling bias happened during the US presidential election in 1936, which pitted Landon against Roosevelt: the Literary Digest conducted a very large poll, sending mail to about 10 million people. It got 2.4 million answers, and predicted with high confidence that Landon would get 57% of the votes. Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest’s sampling method:\n",
        "\n",
        "- First, to obtain the addresses to send the polls to, the Literary Digest used telephone directories, lists of magazine subscribers, club membership lists, and the\n",
        " like. All of these lists tended to favor wealthier people, who were more likely to\n",
        " vote Republican (hence Landon).\n",
        "\n",
        "- Second, less than 25% of the people who were polled answered. Again this introduced a sampling bias, by potentially ruling out people who didn’t care much about politics, people who didn’t like the Literary Digest, and other key groups. This is a special type of sampling bias called nonresponse bias. \n",
        "\n",
        "Here is another example: say you want to build a system to recognize funk music videos. One way to build your training set is to search for \"funk music\" on YouTube and use the resulting videos. But this assumes that YouTube's search engine returns a set of videos that are representative of all the funk music videos on YouTube. In reality, the search results are likely to be biased toward popular artists (and if you live in Brazil you will get a lot of \"funk carioca\" videos, which sound nothing like James Brown). On the other hand, how else can you get a large training set?\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Poor Quality Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Poor-quality data with errors, outliers, or noise reduces model performance, making data cleaning essential a major part of a data scientist’s work. Common approaches include:\n",
        "\n",
        "- Outliers: discard or correct them manually.\n",
        "- Missing features: ignore the attribute, drop affected instances, fill values (e.g., median age), or train models with and without the feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Irrelevant Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A system learns effectively only if training data has enough relevant features and avoids irrelevant ones. Success often depends on feature engineering, which includes:\n",
        "\n",
        "- Feature selection: choosing the most useful existing features.\n",
        "- Feature extraction: combining features to create better ones (e.g., with dimensionality reduction).\n",
        "- Creating new features: gathering additional data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Overfitting the Training Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overgeneralization is common in humans, and Machine Learning models can fall into the same trap—this is called overfitting. A model may perform very well on training data but fail to generalize to new cases. For example, a high-degree polynomial life satisfaction model strongly overfits the training data, fitting noise and irrelevant details rather than real patterns (see Figure 1-22). \n",
        "<br>\n",
        "\n",
        "![Figure 1-22](./Fig/Chapter_1/Fig1-22.png)\n",
        "\n",
        "Complex models such as deep neural networks are especially vulnerable if the training set is noisy or too small, sometimes detecting spurious correlations—for instance, countries with a “w” in their name appearing happier, a coincidence unlikely to generalize.\n",
        "\n",
        "Overfitting occurs when a model is too complex compared to the amount and quality of data. Common solutions include:\n",
        "\n",
        "- **Simplify the model**: reduce parameters, remove attributes, or apply constraints.\n",
        "- **Gather more training** data.\n",
        "- **Reduce data noise**: correct errors and eliminate outliers.\n",
        "\n",
        "Constraining complexity is called regularization. It reduces degrees of freedom, forcing the model to remain simpler and improving its ability to generalize. Figure 1-23 shows how regularization creates a flatter slope: the fit on training data is less precise, but performance on unseen data improves. \n",
        "<br>\n",
        "\n",
        "![Figure 1-23](./Fig/Chapter_1/Fig1-23.png)\n",
        "\n",
        "The amount of regularization is controlled by a hyperparameter, which is set before training and determines the balance between underfitting and overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Underfitting the Training Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Underfitting is the opposite of overfitting: it happens when a model is too simple to capture the true structure of the data. For instance, a linear model of life satisfaction underfits because reality is more complex, leading to inaccurate predictions even on training examples.\n",
        "\n",
        "Possible solutions include:\n",
        "\n",
        "- Use a more powerful model with additional parameters.\n",
        "- Improve features through feature engineering.\n",
        "- Relax model constraints, such as reducing the regularization hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Testing and Validating**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only way to know how well a model generalizes is to test it on new cases. Deploying it in production can work, but if the model performs poorly, users will notice an undesirable approach. A safer method is to split the data into a training set and a test set. The model is trained on the former and evaluated on the latter to estimate the generalization error (also called out-of-sample error), which reflects performance on unseen data.\n",
        "\n",
        "If training error is low but generalization error is high, this indicates the model is overfitting the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Hyperparameter Tuning and Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating a model with a test set is straightforward, but choosing between models or tuning hyperparameters is more complex. For example, comparing a linear and polynomial model can be done with a test set, but repeatedly testing different hyperparameter values (e.g., regularization strength) risks overfitting to the test set itself. This explains why a model that looked good in testing may perform worse in production.\n",
        "\n",
        "The common solution is holdout validation: set aside part of the training set as a validation set (also called dev set). Candidate models with various hyperparameters are trained on the reduced training data and evaluated on the validation set. The best-performing model is then retrained on the full training set, and finally evaluated once on the test set to estimate the generalization error.\n",
        "\n",
        "This approach works well, but the size of the validation set matters. If it is too small, evaluations become noisy and may lead to poor model selection. If it is too large, candidate models are trained on much less data than the final model, reducing fairness. A common remedy is cross-validation, where the training data is split into multiple small validation sets. Each model is trained and evaluated multiple times, and results are averaged, providing a more reliable performance estimate. The drawback is that training time increases proportionally with the number of validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Data Mismatch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes large training datasets are available but are not representative of production data. For example, millions of flower images from the web may differ significantly from photos actually taken with a mobile app. If only 10,000 representative pictures exist, it is crucial that both the validation set and test set consist exclusively of these representative images. Splitting them ensures realistic evaluation and avoids duplicate leakage across sets.\n",
        "\n",
        "If a model trained on web images performs poorly on the validation set, it may be unclear whether this is due to overfitting or a data mismatch. A solution proposed by Andrew Ng is to create a train-dev set from the web data. After training on the remaining web images, evaluate on the train-dev set:\n",
        "\n",
        "- Good performance on train-dev but poor on validation → the issue is data mismatch.\n",
        "- Poor performance on train-dev → the model has overfit the training data.\n",
        "\n",
        "In case of mismatch, preprocessing the web images to resemble mobile app photos can help. If overfitting is the issue, possible remedies include simplifying or regularizing the model, gathering more data, or cleaning the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **No Free Lunch Theorem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. To decide what data to discard and what data to keep, you must make assumptions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.\n",
        "\n",
        "In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you\n",
        "may evaluate various neural networks"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
